{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Slurm - Simple Linux Utility for Resource Management. It is the job scheduler and resource manager which optimize the situations when there is more works than resources, set up the environment for parallel and distributed computing. Development started in 2002 at Lawrence Livermore National. Runs 60% of TOP500 systems. Architecture Central controller daemon - slurmctld - it runs only in the \"Controller Node\" and is responsible for allocating resource to different jobs and also scheduling them. Database daemon - slurmdbd - it runs on the compute nodes and waits to execute work issued by slurmctld. Compute node daemon - slurmd - is the Slurm database and runs only on a single node. It is responsible for account recording information for the multiple Slurm-managed clusters in single database. For full documentation visit slurm workload manager . HPC Slurm Cluster TBD","title":"Overview"},{"location":"#overview","text":"Slurm - Simple Linux Utility for Resource Management. It is the job scheduler and resource manager which optimize the situations when there is more works than resources, set up the environment for parallel and distributed computing. Development started in 2002 at Lawrence Livermore National. Runs 60% of TOP500 systems.","title":"Overview"},{"location":"#architecture","text":"Central controller daemon - slurmctld - it runs only in the \"Controller Node\" and is responsible for allocating resource to different jobs and also scheduling them. Database daemon - slurmdbd - it runs on the compute nodes and waits to execute work issued by slurmctld. Compute node daemon - slurmd - is the Slurm database and runs only on a single node. It is responsible for account recording information for the multiple Slurm-managed clusters in single database. For full documentation visit slurm workload manager .","title":"Architecture"},{"location":"#hpc-slurm-cluster","text":"TBD","title":"HPC Slurm Cluster"},{"location":"commands/","text":"Commands For the summary of Slurm commands please see the link - summary . Job's Status/Resources Reports Status: $ squeue -l -u$USER</span> Details: $ scontrol show job <job-id> The maximal used memory for the completed job: $ sacct -j <job-id> --format=MaxRSS Format options: $ sacct -e Maximal used memory for the running job: $ sstat -j <job-id> --format=MaxRSS When the job will start: $ squeue --start Report status every 60 seconds: $ squeue -i60 You can use the script \"jobinfo\" from GitHub slurm-util site for monitoring your job: $ jobinfo <job-id> Information about computer nodes: $ sinfo More details: $ sinfo -Nel Job's priority","title":"Commands"},{"location":"commands/#commands","text":"For the summary of Slurm commands please see the link - summary .","title":"Commands"},{"location":"commands/#jobs-statusresources-reports","text":"Status: $ squeue -l -u$USER</span> Details: $ scontrol show job <job-id> The maximal used memory for the completed job: $ sacct -j <job-id> --format=MaxRSS Format options: $ sacct -e Maximal used memory for the running job: $ sstat -j <job-id> --format=MaxRSS When the job will start: $ squeue --start Report status every 60 seconds: $ squeue -i60 You can use the script \"jobinfo\" from GitHub slurm-util site for monitoring your job: $ jobinfo <job-id>","title":"Job's Status/Resources Reports"},{"location":"commands/#information-about-computer-nodes","text":"$ sinfo More details: $ sinfo -Nel","title":"Information about computer nodes:"},{"location":"commands/#jobs-priority","text":"","title":"Job's priority"},{"location":"conda_env/","text":"Conda Environment Check that conda is installed $ conda -V Check that conda is up to date $ conda update conda Create virtual environment: $ conda create -n <env-name> Create virtual environment with specific package: $ conda create -n <env-name> scipy Create virtual environment with specific package version: $ conda create -n <env-name> python=3.7 View your environment list: $ conda env list Activate your environment: $ conda activate <env-name> Deactivate your environment: $ conda deactivate Install tensorflow-gpu: $ conda activate <env-name> $ conda install -c anaconda tensorflow-gpu List of packages installed in activated environment: $ conda list List of packages installed in not activated environment: $ conda list -n <env-name> Compare conda environments: $ conda list -n <env-name-1> --export > <file-name-1> $ conda list -n <env-name-2> --export > <file-name-2> $ diff <file-name-1> <file-name-2> Please deactivate your environment before batch file submission.","title":"Conda Environment"},{"location":"conda_env/#conda-environment","text":"Check that conda is installed $ conda -V Check that conda is up to date $ conda update conda Create virtual environment: $ conda create -n <env-name> Create virtual environment with specific package: $ conda create -n <env-name> scipy Create virtual environment with specific package version: $ conda create -n <env-name> python=3.7 View your environment list: $ conda env list Activate your environment: $ conda activate <env-name> Deactivate your environment: $ conda deactivate Install tensorflow-gpu: $ conda activate <env-name> $ conda install -c anaconda tensorflow-gpu List of packages installed in activated environment: $ conda list List of packages installed in not activated environment: $ conda list -n <env-name> Compare conda environments: $ conda list -n <env-name-1> --export > <file-name-1> $ conda list -n <env-name-2> --export > <file-name-2> $ diff <file-name-1> <file-name-2> Please deactivate your environment before batch file submission.","title":"Conda Environment"},{"location":"contact/","text":"Contact If you have any questions or problems with Slurm, please, contact: Inga Paster - Slurm DevOps Engineer: 052-8823-105 61156 paster@bgu.ac.il","title":"Contact"},{"location":"contact/#contact","text":"If you have any questions or problems with Slurm, please, contact: Inga Paster - Slurm DevOps Engineer: 052-8823-105 61156 paster@bgu.ac.il","title":"Contact"},{"location":"faq/","text":"FAQ Why my job is not running? Use the next command to see the reason: $ scontrol show job The reason can be: priority => resources being reserved for higher priority job. resources => required resources are in user. dependency => job dependencies not yet satisfied. reservation => waiting for advanced reservation. AssociationJobLimit => user account job limit reached. AssociationResourceLimit => user account resource limit reached. AssociationTimeLimit => user account time limit reached. QOSJobLimit => Quality Of Service (QOS) job limit reached. QOSResourceLimit => Quality Of Service (QOS) resource limit reached. QOSTimeLimit => Quality Of Service (QOS) time limit reached. Example: Why my job was killed? Use #SBATCH --requeue option in your Batch File. Why sstat doesn't show the used resources of my completed job? sstat command need be used for the running job_id. for completed jobs use sacct command. Why sacct doesn't show the used resources of my running job? sacct command need be used for the completed job_id. for the running jobs use sstat command. sacct command need to run on the master node only, so to execute it it need be written in the script.","title":"FAQ"},{"location":"faq/#faq","text":"Why my job is not running? Use the next command to see the reason: $ scontrol show job The reason can be: priority => resources being reserved for higher priority job. resources => required resources are in user. dependency => job dependencies not yet satisfied. reservation => waiting for advanced reservation. AssociationJobLimit => user account job limit reached. AssociationResourceLimit => user account resource limit reached. AssociationTimeLimit => user account time limit reached. QOSJobLimit => Quality Of Service (QOS) job limit reached. QOSResourceLimit => Quality Of Service (QOS) resource limit reached. QOSTimeLimit => Quality Of Service (QOS) time limit reached. Example: Why my job was killed? Use #SBATCH --requeue option in your Batch File. Why sstat doesn't show the used resources of my completed job? sstat command need be used for the running job_id. for completed jobs use sacct command. Why sacct doesn't show the used resources of my running job? sacct command need be used for the completed job_id. for the running jobs use sstat command. sacct command need to run on the master node only, so to execute it it need be written in the script.","title":"FAQ"},{"location":"get_started/","text":"Connect to HPC Slurm Cluster TBD","title":"Connection to HPC Slurm"},{"location":"get_started/#connect-to-hpc-slurm-cluster","text":"TBD","title":"Connect to HPC Slurm Cluster"},{"location":"jupyter/","text":"Jupyter TBD","title":"Jupyter"},{"location":"jupyter/#jupyter","text":"TBD","title":"Jupyter"},{"location":"matlab/","text":"Matlab TBD","title":"Matlab"},{"location":"matlab/#matlab","text":"TBD","title":"Matlab"},{"location":"pyCharm/","text":"pyCharm TBD","title":"pyCharm"},{"location":"pyCharm/#pycharm","text":"TBD","title":"pyCharm"},{"location":"sbatch/","text":"SBatch The job flags are used with #SBATCH command. The syntax for the SLURM directive in a script is \" #SBATCH < flag >\". Some of the flags are used with the srun and salloc commands, as well as the fisbatch wrapper script for interactive jobs. Sbatch configuration parameters must start with #SBATCH and must precede any other command. #SBATCH - Bash \"sees\" this as comment. #SBATCH - Slurm \"takes\" this as parameter. Use ## to close #SBATCH as comment. Example of SBatch Submition Script (Batch File): NOTE : the term \"Batch File\" is used throughout this documentation to mean an executable file that you create and submit to the job scheduler to run on a node or collection of nodes. This script will include a list of Slurm directives (or commands) to tell the job scheduler what to do. Details and options for these scripts are below. Additional Flags' Description Flag property Description property Default #SBATCH -- partition = TBD Partition name. TBD #SBATCH -- time =0-12:00:00 Limit the time of job running. Format: D-H:MM:SS. TBD #SBATCH -- job-name =my_job Name of the job. Replace my_job with your desired job name. #SBATCH -- output =%a_my_name-%J.out Output log for running job, %J \u2013 job #, %a \u2013 run #. #SBATCH -- mail-user = username@post.bgu.ac.il Users email for sending job status notifications. #SBATCH -- mail-type =BEGIN,END,FAIL Conditions when to send the email. ALL,BEGIN,END,FAIL, REQUEU, NONE. #SBATCH -- nodelist = TBD Specify the computer to run the job. TBD #SBATCH -- array =1-10 Run parallel 10 times. TBD #SBATCH -- mem =72G Allocate extra memory. TBD #SBATCH -- gres =gpu:1 Number of GPUs (can't exceed 8 gpus for now) ask for more than 1 only if you can parallelize your code for multi GPU. TBD #SBATCH -- nodes =1-1 Allocate 1 node. TBD #SBATCH -- ntasks =12 Allocate 12 tasks TBD #SBATCH -- cpus-per-task =32 Allocate extra CPU: 32 CPU per task. TBD #SBATCH -- requeue =32 Re-run the task if it was preempted. TBD #SBATCH -- dependency =after(ok/notok/any): Ordering of jobs. TBD #SBATCH -- exclusive = To reserve a whole node for yourself, slurm will reserve a full node for the first job to start in the array, and then will schedule all the others on the same machine as only your jobs will be allowed there. TBD #SBATCH %j Job #. #SBATCH %A Value of SLURM_ARRAY_JOB_ID. #SBATCH %a Value of SLURM_ARRAY_TASK_ID. Submit Job $ sbatch ./<sbatch-file-name> If you have QOS privileges, you can use: $ sbatch --qos=<user-name> ./<sbatch-file-name> After job submission, Slurm gives the JobID, to see it use: $ ls -lrt Cancel Job $ scancel <job-id> or $ scancel --name <job-name> For pending jobs pleas use: $ scancel -t PENDING -u <user-name> View Job ID and it's Status $ squeue -l -u$USER or $ squeue -l -u<user-name> For description of codes of job state please use: $ man squeue Some of them are: R - running PD - pending CA - canceled CD - completed F - failed","title":"SBatch"},{"location":"sbatch/#sbatch","text":"The job flags are used with #SBATCH command. The syntax for the SLURM directive in a script is \" #SBATCH < flag >\". Some of the flags are used with the srun and salloc commands, as well as the fisbatch wrapper script for interactive jobs. Sbatch configuration parameters must start with #SBATCH and must precede any other command. #SBATCH - Bash \"sees\" this as comment. #SBATCH - Slurm \"takes\" this as parameter. Use ## to close #SBATCH as comment.","title":"SBatch"},{"location":"sbatch/#example-of-sbatch-submition-script-batch-file","text":"NOTE : the term \"Batch File\" is used throughout this documentation to mean an executable file that you create and submit to the job scheduler to run on a node or collection of nodes. This script will include a list of Slurm directives (or commands) to tell the job scheduler what to do. Details and options for these scripts are below.","title":"Example of SBatch Submition Script (Batch File):"},{"location":"sbatch/#additional-flags-description","text":"Flag property Description property Default #SBATCH -- partition = TBD Partition name. TBD #SBATCH -- time =0-12:00:00 Limit the time of job running. Format: D-H:MM:SS. TBD #SBATCH -- job-name =my_job Name of the job. Replace my_job with your desired job name. #SBATCH -- output =%a_my_name-%J.out Output log for running job, %J \u2013 job #, %a \u2013 run #. #SBATCH -- mail-user = username@post.bgu.ac.il Users email for sending job status notifications. #SBATCH -- mail-type =BEGIN,END,FAIL Conditions when to send the email. ALL,BEGIN,END,FAIL, REQUEU, NONE. #SBATCH -- nodelist = TBD Specify the computer to run the job. TBD #SBATCH -- array =1-10 Run parallel 10 times. TBD #SBATCH -- mem =72G Allocate extra memory. TBD #SBATCH -- gres =gpu:1 Number of GPUs (can't exceed 8 gpus for now) ask for more than 1 only if you can parallelize your code for multi GPU. TBD #SBATCH -- nodes =1-1 Allocate 1 node. TBD #SBATCH -- ntasks =12 Allocate 12 tasks TBD #SBATCH -- cpus-per-task =32 Allocate extra CPU: 32 CPU per task. TBD #SBATCH -- requeue =32 Re-run the task if it was preempted. TBD #SBATCH -- dependency =after(ok/notok/any): Ordering of jobs. TBD #SBATCH -- exclusive = To reserve a whole node for yourself, slurm will reserve a full node for the first job to start in the array, and then will schedule all the others on the same machine as only your jobs will be allowed there. TBD #SBATCH %j Job #. #SBATCH %A Value of SLURM_ARRAY_JOB_ID. #SBATCH %a Value of SLURM_ARRAY_TASK_ID.","title":"Additional Flags' Description"},{"location":"sbatch/#submit-job","text":"$ sbatch ./<sbatch-file-name> If you have QOS privileges, you can use: $ sbatch --qos=<user-name> ./<sbatch-file-name> After job submission, Slurm gives the JobID, to see it use: $ ls -lrt","title":"Submit Job"},{"location":"sbatch/#cancel-job","text":"$ scancel <job-id> or $ scancel --name <job-name> For pending jobs pleas use: $ scancel -t PENDING -u <user-name>","title":"Cancel Job"},{"location":"sbatch/#view-job-id-and-its-status","text":"$ squeue -l -u$USER or $ squeue -l -u<user-name> For description of codes of job state please use: $ man squeue Some of them are: R - running PD - pending CA - canceled CD - completed F - failed","title":"View Job ID and it's Status"},{"location":"srun/","text":"SRun TBD","title":"SRun"},{"location":"srun/#srun","text":"TBD","title":"SRun"}]}