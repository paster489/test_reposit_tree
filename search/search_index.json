{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Slurm - Simple Linux Utility for Resource Management. It is the job scheduler and resource manager which optimize the situations when there is more works than resources, set up the environment for parallel and distributed computing. Development started in 2002 at Lawrence Livermore National. Runs 60% of TOP500 systems. Architecture Central controller daemon - slurmctld - it runs only in the \"Controller Node\" and is responsible for allocating resource to different jobs and also scheduling them. Database daemon - slurmdbd - it runs on the compute nodes and waits to execute work issued by slurmctld. Compute node daemon - slurmd - is the Slurm database and runs only on a single node. It is responsible for account recording information for the multiple Slurm-managed clusters in single database. For full documentation visit slurm workload manager . HPC Slurm Cluster TBD","title":"Overview"},{"location":"#overview","text":"Slurm - Simple Linux Utility for Resource Management. It is the job scheduler and resource manager which optimize the situations when there is more works than resources, set up the environment for parallel and distributed computing. Development started in 2002 at Lawrence Livermore National. Runs 60% of TOP500 systems.","title":"Overview"},{"location":"#architecture","text":"Central controller daemon - slurmctld - it runs only in the \"Controller Node\" and is responsible for allocating resource to different jobs and also scheduling them. Database daemon - slurmdbd - it runs on the compute nodes and waits to execute work issued by slurmctld. Compute node daemon - slurmd - is the Slurm database and runs only on a single node. It is responsible for account recording information for the multiple Slurm-managed clusters in single database. For full documentation visit slurm workload manager .","title":"Architecture"},{"location":"#hpc-slurm-cluster","text":"TBD","title":"HPC Slurm Cluster"},{"location":"commands/","text":"Commands For the summary of Slurm commands please see the link - summary . Jobs Report status of submitted job: $ squeue -l -u$USER or $ squeue -l -u<user-name> Report status of submitted job every 60 seconds: $ squeue -i60 -l -u$USER Details of submitted job: $ scontrol show job <job-id> The maximal memory used for the completed job: $ sacct -j <job-id> --format=MaxRSS Maximal memory used for the running job: $ sstat -j <job-id> --format=MaxRSS To see all format options: $ sacct -e or $ sstat -e To see when the submitted job will start: $ squeue --start In addition, you can use the script \"jobinfo\" from GitHub slurm-util site for monitoring your jobs. To run the script, save it in your directory and write the command: $ <path-to-file>/jobinfo <job-id> Resources To see cluster partitioning: $ sinfo More details about cluster partitioning: $ sinfo -Nel or $ scontrol show partition See availible GPUs: $ sres See Slurm account information: $ sacctmgr show <option> Options: \"Account\", \"Association\", \"Cluster\", \"Configuration\", \"Event\", \"Federation\", \"Problem\", \"QOS\", \"Resource\", \"Reservation\", \"RunAwayJobs\", \"Stats\", \"Transaction\", \"TRES\", \"User\", or \"WCKey\". See help for details. Limits See resource limit configuration: $ sacctmgr show assoc_mgr See the limit of tasks which user can run: $ sacctmgr show qos|grep <partition-name> Priority To see how defined your QOS priority: $ sacctmgr list assoc user=<user-name> format=Cluster,Account,User,QOS,defaultqos See account and fair-share parameters: $ sshare -l Run the job with QOS priority privilege: $ sbatch --qos=<priority-user-name> <path-to-file>/<sbatch-file-name> Piping Start job after started: $ sbatch --depend=after:<other-job-id> <path-to-file>/<sbatch-file-name> Start job after ends with ok status (the options can be (ok|notok|any): $ sbatch --depend=afterok:<other_job_id> <path-to-file>/<sbatch-file-name> Start after both job 77 and 79 have finished: $ sbatch --depend=afterok:77:79 <path-to-file>/<sbatch-file-name>","title":"Commands"},{"location":"commands/#commands","text":"For the summary of Slurm commands please see the link - summary .","title":"Commands"},{"location":"commands/#jobs","text":"Report status of submitted job: $ squeue -l -u$USER or $ squeue -l -u<user-name> Report status of submitted job every 60 seconds: $ squeue -i60 -l -u$USER Details of submitted job: $ scontrol show job <job-id> The maximal memory used for the completed job: $ sacct -j <job-id> --format=MaxRSS Maximal memory used for the running job: $ sstat -j <job-id> --format=MaxRSS To see all format options: $ sacct -e or $ sstat -e To see when the submitted job will start: $ squeue --start In addition, you can use the script \"jobinfo\" from GitHub slurm-util site for monitoring your jobs. To run the script, save it in your directory and write the command: $ <path-to-file>/jobinfo <job-id>","title":"Jobs"},{"location":"commands/#resources","text":"To see cluster partitioning: $ sinfo More details about cluster partitioning: $ sinfo -Nel or $ scontrol show partition See availible GPUs: $ sres See Slurm account information: $ sacctmgr show <option> Options: \"Account\", \"Association\", \"Cluster\", \"Configuration\", \"Event\", \"Federation\", \"Problem\", \"QOS\", \"Resource\", \"Reservation\", \"RunAwayJobs\", \"Stats\", \"Transaction\", \"TRES\", \"User\", or \"WCKey\". See help for details.","title":"Resources"},{"location":"commands/#limits","text":"See resource limit configuration: $ sacctmgr show assoc_mgr See the limit of tasks which user can run: $ sacctmgr show qos|grep <partition-name>","title":"Limits"},{"location":"commands/#priority","text":"To see how defined your QOS priority: $ sacctmgr list assoc user=<user-name> format=Cluster,Account,User,QOS,defaultqos See account and fair-share parameters: $ sshare -l Run the job with QOS priority privilege: $ sbatch --qos=<priority-user-name> <path-to-file>/<sbatch-file-name>","title":"Priority"},{"location":"commands/#piping","text":"Start job after started: $ sbatch --depend=after:<other-job-id> <path-to-file>/<sbatch-file-name> Start job after ends with ok status (the options can be (ok|notok|any): $ sbatch --depend=afterok:<other_job_id> <path-to-file>/<sbatch-file-name> Start after both job 77 and 79 have finished: $ sbatch --depend=afterok:77:79 <path-to-file>/<sbatch-file-name>","title":"Piping"},{"location":"conda_env/","text":"Conda Environment Check that conda is installed $ conda -V Check that conda is up to date $ conda update conda Create virtual environment: $ conda create -n <env-name> Create virtual environment with specific package: $ conda create -n <env-name> scipy Create virtual environment with specific package version: $ conda create -n <env-name> python=3.7 View your environment list: $ conda env list Activate your environment: $ conda activate <env-name> Deactivate your environment: $ conda deactivate Install tensorflow-gpu: $ conda activate <env-name> $ conda install -c anaconda tensorflow-gpu Install ipykernel: $ python -m ipykernel install --user --name <env-name> --display-name \"<env-name-to-display>\" List of packages installed in activated environment: $ conda list List of packages installed in not activated environment: $ conda list -n <env-name> Compare conda environments: $ conda list -n <env-name-1> --export > <file-name-1> $ conda list -n <env-name-2> --export > <file-name-2> $ diff <file-name-1> <file-name-2> Please deactivate your environment before batch file submission.","title":"Conda Environment"},{"location":"conda_env/#conda-environment","text":"Check that conda is installed $ conda -V Check that conda is up to date $ conda update conda Create virtual environment: $ conda create -n <env-name> Create virtual environment with specific package: $ conda create -n <env-name> scipy Create virtual environment with specific package version: $ conda create -n <env-name> python=3.7 View your environment list: $ conda env list Activate your environment: $ conda activate <env-name> Deactivate your environment: $ conda deactivate Install tensorflow-gpu: $ conda activate <env-name> $ conda install -c anaconda tensorflow-gpu Install ipykernel: $ python -m ipykernel install --user --name <env-name> --display-name \"<env-name-to-display>\" List of packages installed in activated environment: $ conda list List of packages installed in not activated environment: $ conda list -n <env-name> Compare conda environments: $ conda list -n <env-name-1> --export > <file-name-1> $ conda list -n <env-name-2> --export > <file-name-2> $ diff <file-name-1> <file-name-2> Please deactivate your environment before batch file submission.","title":"Conda Environment"},{"location":"connection/","text":"Connect to HPC Slurm Cluster TBD","title":"Connection to HPC Slurm"},{"location":"connection/#connect-to-hpc-slurm-cluster","text":"TBD","title":"Connect to HPC Slurm Cluster"},{"location":"contact/","text":"Contact If you have any questions or problems with Slurm, please, contact: Inga Paster - Slurm DevOps Engineer: Cell phone - 052-8823-105 Office - 61156 E-mail - paster@post.bgu.ac.il","title":"Contact"},{"location":"contact/#contact","text":"If you have any questions or problems with Slurm, please, contact: Inga Paster - Slurm DevOps Engineer: Cell phone - 052-8823-105 Office - 61156 E-mail - paster@post.bgu.ac.il","title":"Contact"},{"location":"faq/","text":"FAQ Why my job is not running? Use the next command to see the reason: $ scontrol show job <job-id> The reason can be: priority => resources being reserved for higher priority job. resources => required resources are in user. dependency => job dependencies not yet satisfied. reservation => waiting for advanced reservation. AssociationJobLimit => user account job limit reached. AssociationResourceLimit => user account resource limit reached. AssociationTimeLimit => user account time limit reached. QOSJobLimit => Quality Of Service (QOS) job limit reached. QOSResourceLimit => Quality Of Service (QOS) resource limit reached. QOSTimeLimit => Quality Of Service (QOS) time limit reached. Example: Why my job was killed? Use the SBatch Flag below in your Batch File. #SBATCH --requeue Why sstat doesn't work? sstat command need be used for the running job_id. for completed jobs use sacct command. Why sacct doesn't work? sacct command need be used for the completed job_id. for the running jobs use sstat command. sacct command need to run on the master node only, so to execute it it need be written in the script. What to choose --ntasks or --array? Let's review two schemes: in the first one we are defining one job with 8 tasks: \"srun echo hello\". In the second scheme: we reserve in Slurm one job with one task and array = 1-8. So what is the difference between these two options? In the Scheme 1: all the tasks will be scheduled at the same time . So it will need 8 CPUS to become available at the same time. This is typically only used with parallel jobs where processes need to communicate with each others, for instance using an Message Passing Interface library. In the Scheme 2: the 8 tasks will be scheduled independently one of another. If 8 CPUs become available at once, they will all start at the same time, but if only 4 CPUs become available at first, 4 tasks will run, the other 4 remaining pending. It is typically used in the case of embarrassingly parallel jobs, where the processes do not need to communicate or synchronize, like for applying the same program to a list of files. How to know if I am working with GPU or CPU? In order to test if your GPU is in use and working please run the next scrip from BGU ISE web site.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#why-my-job-is-not-running","text":"Use the next command to see the reason: $ scontrol show job <job-id> The reason can be: priority => resources being reserved for higher priority job. resources => required resources are in user. dependency => job dependencies not yet satisfied. reservation => waiting for advanced reservation. AssociationJobLimit => user account job limit reached. AssociationResourceLimit => user account resource limit reached. AssociationTimeLimit => user account time limit reached. QOSJobLimit => Quality Of Service (QOS) job limit reached. QOSResourceLimit => Quality Of Service (QOS) resource limit reached. QOSTimeLimit => Quality Of Service (QOS) time limit reached. Example:","title":"Why my job is not running?"},{"location":"faq/#why-my-job-was-killed","text":"Use the SBatch Flag below in your Batch File. #SBATCH --requeue","title":"Why my job was killed?"},{"location":"faq/#why-sstat-doesnt-work","text":"sstat command need be used for the running job_id. for completed jobs use sacct command.","title":"Why sstat doesn't work?"},{"location":"faq/#why-sacct-doesnt-work","text":"sacct command need be used for the completed job_id. for the running jobs use sstat command. sacct command need to run on the master node only, so to execute it it need be written in the script.","title":"Why sacct doesn't work?"},{"location":"faq/#what-to-choose-ntasks-or-array","text":"Let's review two schemes: in the first one we are defining one job with 8 tasks: \"srun echo hello\". In the second scheme: we reserve in Slurm one job with one task and array = 1-8. So what is the difference between these two options? In the Scheme 1: all the tasks will be scheduled at the same time . So it will need 8 CPUS to become available at the same time. This is typically only used with parallel jobs where processes need to communicate with each others, for instance using an Message Passing Interface library. In the Scheme 2: the 8 tasks will be scheduled independently one of another. If 8 CPUs become available at once, they will all start at the same time, but if only 4 CPUs become available at first, 4 tasks will run, the other 4 remaining pending. It is typically used in the case of embarrassingly parallel jobs, where the processes do not need to communicate or synchronize, like for applying the same program to a list of files.","title":"What to choose --ntasks or --array?"},{"location":"faq/#how-to-know-if-i-am-working-with-gpu-or-cpu","text":"In order to test if your GPU is in use and working please run the next scrip from BGU ISE web site.","title":"How to know if I am working with GPU or CPU?"},{"location":"jupyter/","text":"Jupyter Instaltion Check that python , ipython , ipykernel (interactive python shell used for Jupyter kernel) and jupyterlab packages are installed in your environment (see Conda Environment section for details about the installation). Batch File Submission Submit Batch File with the next executions in the end of the file: module load anaconda source activate <env-name> running the job jupyter lab URL for Jupyter web access After the job was successfully submitted and has the status of running, wait for 1 minute and then open output log file of the job and copy 2 nd token: https://132/72.x.x. and paste it into your web browser\u2019s address bar. In the open Jupyter notebook check that the correct environment name is chosen (upper right corner).","title":"Jupyter"},{"location":"jupyter/#jupyter","text":"","title":"Jupyter"},{"location":"jupyter/#instaltion","text":"Check that python , ipython , ipykernel (interactive python shell used for Jupyter kernel) and jupyterlab packages are installed in your environment (see Conda Environment section for details about the installation).","title":"Instaltion"},{"location":"jupyter/#batch-file-submission","text":"Submit Batch File with the next executions in the end of the file: module load anaconda source activate <env-name> running the job jupyter lab","title":"Batch File Submission"},{"location":"jupyter/#url-for-jupyter-web-access","text":"After the job was successfully submitted and has the status of running, wait for 1 minute and then open output log file of the job and copy 2 nd token: https://132/72.x.x. and paste it into your web browser\u2019s address bar. In the open Jupyter notebook check that the correct environment name is chosen (upper right corner).","title":"URL for Jupyter web access"},{"location":"matlab/","text":"Matlab TBD","title":"Matlab"},{"location":"matlab/#matlab","text":"TBD","title":"Matlab"},{"location":"pyCharm/","text":"pyCharm TBD","title":"pyCharm"},{"location":"pyCharm/#pycharm","text":"TBD","title":"pyCharm"},{"location":"sbatch/","text":"SBATCH The syntax for the SLURM directive in a script is \" #SBATCH < flag >\". Some of the flags are used with the srun and salloc commands, as well as the fisbatch wrapper script for interactive jobs. Sbatch configuration parameters must start with #SBATCH and must precede any other command. #SBATCH - Bash \"sees\" this as comment. #SBATCH - Slurm \"takes\" this as parameter. Use ## to close #SBATCH as comment. Example of SBatch Submition Script (Batch File): NOTE : the term \"Batch File\" is used throughout this documentation to mean an executable file that you create and submit to the job scheduler to run on a node or collection of nodes. This script will include a list of Slurm directives (or commands) to tell the job scheduler what to do. Additional Flags' Description Flag property Description property Default #SBATCH -- partition = TBD Partition name. TBD #SBATCH -- time =0-12:00:00 Limit the time of job running. Format: D-H:MM:SS. TBD #SBATCH -- job-name =my_job Name of the job. Replace my_job with your desired job name. #SBATCH -- output =%a_my_name-%J.out Output log for running job, %J \u2013 job #, %a \u2013 run #. #SBATCH -- mail-user = username@post.bgu.ac.il Users email for sending job status notifications. #SBATCH -- mail-type =BEGIN,END,FAIL Conditions when to send the email. ALL,BEGIN,END,FAIL, REQUEU, NONE. #SBATCH -- nodelist = TBD Specify the computer to run the job. TBD #SBATCH -- array =1-10 Run parallel 10 times. TBD #SBATCH -- mem =72G Allocate extra memory. TBD #SBATCH -- gres =gpu:1 Number of GPUs (can't exceed 8 gpus for now) ask for more than 1 only if you can parallelize your code for multi GPU. TBD #SBATCH -- nodes =1-1 Allocate 1 node. TBD #SBATCH -- ntasks =12 Allocate 12 tasks TBD #SBATCH -- cpus-per-task =32 Allocate extra CPU: 32 CPU per task. TBD #SBATCH -- requeue =32 Re-run the task if it was preempted. TBD #SBATCH -- dependency =after(ok/notok/any): Ordering of jobs. TBD #SBATCH -- exclusive = To reserve a whole node for yourself, slurm will reserve a full node for the first job to start in the array, and then will schedule all the others on the same machine as only your jobs will be allowed there. TBD #SBATCH %j Job #. #SBATCH %A Value of SLURM_ARRAY_JOB_ID. #SBATCH %a Value of SLURM_ARRAY_TASK_ID. Submit Job $ sbatch <path-to-file>/<sbatch-file-name> If you have QOS privileges, you can use: $ sbatch --qos=<priority-user-name> <path-to-file>/<sbatch-file-name> After job submission, Slurm gives the JobID, to see it use: $ ls -lrt Cancel Job Cancel the specific job: $ scancel <job-id> or $ scancel --name <job-name> Cancel all jobs of the user: $ scancel -u<user-name> or $ scancel -u$USER Cancel pending jobs of the user: $ scancel -t PENDING -u <user-name> View Job ID and it's Status $ squeue -l -u$USER or $ squeue -l -u<user-name> For codes of job state description please use: $ man squeue Some of them are: R - running PD - pending CA - canceled CD - completed F - failed","title":"SBATCH"},{"location":"sbatch/#sbatch","text":"The syntax for the SLURM directive in a script is \" #SBATCH < flag >\". Some of the flags are used with the srun and salloc commands, as well as the fisbatch wrapper script for interactive jobs. Sbatch configuration parameters must start with #SBATCH and must precede any other command. #SBATCH - Bash \"sees\" this as comment. #SBATCH - Slurm \"takes\" this as parameter. Use ## to close #SBATCH as comment.","title":"SBATCH"},{"location":"sbatch/#example-of-sbatch-submition-script-batch-file","text":"NOTE : the term \"Batch File\" is used throughout this documentation to mean an executable file that you create and submit to the job scheduler to run on a node or collection of nodes. This script will include a list of Slurm directives (or commands) to tell the job scheduler what to do.","title":"Example of SBatch Submition Script (Batch File):"},{"location":"sbatch/#additional-flags-description","text":"Flag property Description property Default #SBATCH -- partition = TBD Partition name. TBD #SBATCH -- time =0-12:00:00 Limit the time of job running. Format: D-H:MM:SS. TBD #SBATCH -- job-name =my_job Name of the job. Replace my_job with your desired job name. #SBATCH -- output =%a_my_name-%J.out Output log for running job, %J \u2013 job #, %a \u2013 run #. #SBATCH -- mail-user = username@post.bgu.ac.il Users email for sending job status notifications. #SBATCH -- mail-type =BEGIN,END,FAIL Conditions when to send the email. ALL,BEGIN,END,FAIL, REQUEU, NONE. #SBATCH -- nodelist = TBD Specify the computer to run the job. TBD #SBATCH -- array =1-10 Run parallel 10 times. TBD #SBATCH -- mem =72G Allocate extra memory. TBD #SBATCH -- gres =gpu:1 Number of GPUs (can't exceed 8 gpus for now) ask for more than 1 only if you can parallelize your code for multi GPU. TBD #SBATCH -- nodes =1-1 Allocate 1 node. TBD #SBATCH -- ntasks =12 Allocate 12 tasks TBD #SBATCH -- cpus-per-task =32 Allocate extra CPU: 32 CPU per task. TBD #SBATCH -- requeue =32 Re-run the task if it was preempted. TBD #SBATCH -- dependency =after(ok/notok/any): Ordering of jobs. TBD #SBATCH -- exclusive = To reserve a whole node for yourself, slurm will reserve a full node for the first job to start in the array, and then will schedule all the others on the same machine as only your jobs will be allowed there. TBD #SBATCH %j Job #. #SBATCH %A Value of SLURM_ARRAY_JOB_ID. #SBATCH %a Value of SLURM_ARRAY_TASK_ID.","title":"Additional Flags' Description"},{"location":"sbatch/#submit-job","text":"$ sbatch <path-to-file>/<sbatch-file-name> If you have QOS privileges, you can use: $ sbatch --qos=<priority-user-name> <path-to-file>/<sbatch-file-name> After job submission, Slurm gives the JobID, to see it use: $ ls -lrt","title":"Submit Job"},{"location":"sbatch/#cancel-job","text":"Cancel the specific job: $ scancel <job-id> or $ scancel --name <job-name> Cancel all jobs of the user: $ scancel -u<user-name> or $ scancel -u$USER Cancel pending jobs of the user: $ scancel -t PENDING -u <user-name>","title":"Cancel Job"},{"location":"sbatch/#view-job-id-and-its-status","text":"$ squeue -l -u$USER or $ squeue -l -u<user-name> For codes of job state description please use: $ man squeue Some of them are: R - running PD - pending CA - canceled CD - completed F - failed","title":"View Job ID and it's Status"},{"location":"srun/","text":"SRUN Slurm jobs are normally batch jobs in the sense that they are run unattended. If I need iterative bash session you can use srun option. $ srun --mem=32G --cpus-per-task=6 -n 4 <command> or $ srun --mem=32G --cpus-per-task=6 -n 4 <path-to-file>/<file-name> The next brief explanation of srun evaluation was taken from Slurm home page: See the next link about srun details.","title":"SRUN"},{"location":"srun/#srun","text":"Slurm jobs are normally batch jobs in the sense that they are run unattended. If I need iterative bash session you can use srun option. $ srun --mem=32G --cpus-per-task=6 -n 4 <command> or $ srun --mem=32G --cpus-per-task=6 -n 4 <path-to-file>/<file-name> The next brief explanation of srun evaluation was taken from Slurm home page: See the next link about srun details.","title":"SRUN"}]}